

Lake et al. (2015)
see the capacity for one-shot generalization demonstrated
by Bayesian programming learning ‘as a challenge for neural
models’. By combining the representational power of
deep neural networks embedded within hierarchical latent
variable models, with the inferential power of approximate
Bayesian reasoning, we show that this is a challenge that
can be overcome.

Two principles are central to our approach: feedback and
attention. These principles allow the models we develop to
reflect the principles of analysis-by-synthesis, in which the
analysis of observed information is continually integrated
with constructed interpretations of it


[What is DRAW (Deep Recurrent Attentive Writer)?](http://kvfrans.com/what-is-draw-deep-recurrent-attentive-writer/)
[One Shot Generalization](https://casmls.github.io/general/2017/02/08/oneshot.html)